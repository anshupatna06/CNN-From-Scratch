# CNN-From-Scratch
"DL models implemented from scratch using NumPy and Pandas only"
# ğŸ“Œ CNN Building Blocks From Scratch (Deep Learning Fundamentals)

A complete low-level implementation of the core operations behind Convolutional Neural Networks â€” implemented entirely from scratch using NumPy without any DL framework.

This project demonstrates:

âœ”ï¸ Mathematical understanding

âœ”ï¸ Low-level architecture fundamentals

âœ”ï¸ Backpropagation logic

âœ”ï¸ im2col & col2im vectorization

âœ”ï¸ How CNNs actually â€œseeâ€ images

âœ”ï¸ Efficient computation with matrix operations

This repository is your foundation to later build a full CNN from scratch (MNIST, CIFAR-10 etc.).

## â­ 1. What is Implemented Here?
### ğŸ”¹ 1. Convolution Forward Pass (From Scratch)

Using im2col for efficient patch extraction.

Mathematically:
$$\text{out}[n, f, h, w]$$ = $$\sum_{c=0}^{C-1} \sum_{i=0}^{FH-1} \sum_{j=0}^{FW-1}
X[n, c, h+i, w+j] \cdot W[f, c, i, j]$$

### ğŸ”¹ 2. Convolution Backward Pass (From Scratch)

Computes gradients:

Gradient wrt output: dout

Gradient wrt weights:


$$dW[f, c, i, j]$$ = $$\sum_{n,h,w} X[n,c,h+i,w+j] \cdot dOut[n,f,h,w]$$

Gradient wrt input:


dX = $$\text{col2im}(dX_{col})$$


---

### ğŸ”¹ 3. im2col Implementation

Transforms patches â†’ columns to convert convolution into matrix multiplication.

Visually:

Image (HÃ—WÃ—C)
  â†“ patches
im2col â†’ matrix (C*FH*FW , H_out * W_out)

This:

Speeds computation

Avoids nested loops

Makes convolution = dot product

### ğŸ”¹ 4. col2im Implementation

The inverse of im2col â€” required for backward propagation to reconstruct dX.

## â­ 2. Why This Repository Matters

Convolution layers in PyTorch/TensorFlow are black boxes.

This project reveals whatâ€™s inside:

âœ” How CNNs extract edges, textures, patterns
âœ” How filters slide over images
âœ” How backprop updates filters
âœ” How gradients flow
âœ” How patches are vectorized

This level of depth is what engineers working at Microsoft, Google, Meta understand.

This repo demonstrates that you understand CNNs far below the surface.

## â­ 3. Folder Structure Explained
ğŸ“ src/

Low-level implementations:

File	Description
conv_forward.py	Convolution forward pass
conv_backward.py	Backpropagation for convolution
im2col.py	Convert image â†’ patches
col2im.py	Convert patches â†’ image
utils.py	Utility functions
test_convolution.py	Basic correctness tests
ğŸ“ notebooks/

Contains visual demos using Matplotlib.

ğŸ“ visuals/

Contains diagrams for readme:

im2col explained

convolution operation diagram

gradient flow

shape transformations

## â­ 4. Key Mathematical Formulas
ğŸ”¹ Output Shape of Convolution
$$H_{out}$$ = $$\frac{H + 2P - FH}{S} + 1$$

$$W_{out}$$ = $$\frac{W + 2P - FW}{S} + 1$$


---

ğŸ”¹ Convolution as Matrix Multiplication

$$X_{col} \in \mathbb{R}^{(C \cdot FH \cdot FW) \times (H_{out} \cdot W_{out})}$$

$$W_{row} \in \mathbb{R}^{F \times (C \cdot FH \cdot FW)}$$

Out = $$W_{row} \cdot X_{col}$$


---

ğŸ”¹ Weight Gradient

dW = $$dOut \cdot X_{col}^{T}$$


---

ğŸ”¹ Input Gradient

dX = $$col2im(W^T \cdot dOut)$$
## â­ 5. Running the Demo
pip install -r requirements.txt


Then:

python src/test_convolution.py


Or run Jupyter notebook:

jupyter notebook notebooks/CNN_Convolution_Visual_Demo.ipynb

## â­ 6. Roadmap (Coming Next)

You will soon build:

âœ” MaxPooling from scratch

âœ” ReLU + Softmax

âœ” Fully Connected Layer

âœ” Mini-Batch Gradient Descent

âœ” Adam Optimizer

âœ” Full CNN Training on MNIST

âœ” Visualizing learned filters

## â­ 7. Author

Anshu Pandey
Machine Learning & Deep Learning Learner
Target â€” Microsoft AI/ML Internship
